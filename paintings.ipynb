{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b9fcd3a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os, random, zipfile, itertools, math, shutil, time\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "902bb178",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 256\n",
    "def pil_to_tensor(img: Image.Image) -> torch.Tensor:\n",
    "    img = img.resize((IMG_SIZE, IMG_SIZE), Image.BICUBIC)\n",
    "    arr = np.asarray(img, dtype=np.float32) / 255.0 \n",
    "    arr = arr * 2.0 - 1.0 \n",
    "    return torch.from_numpy(arr).permute(2,0,1)\n",
    "\n",
    "def tensor_to_pil(t: torch.Tensor) -> Image.Image:\n",
    "    t = (t.clamp(-1,1) + 1.0) * 0.5    \n",
    "    arr = (t.cpu().numpy().transpose(1,2,0) * 255.0).astype(np.uint8)\n",
    "    return Image.fromarray(arr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "36ff5c71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 300 Monet and 7038 photo images.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ROOT = Path.cwd() / \"gan-getting-started\"  \n",
    "MONET_DIR = ROOT / \"monet_jpg\"\n",
    "PHOTO_DIR = ROOT / \"photo_jpg\"\n",
    "GEN_DIR   = ROOT / \"gen\"  \n",
    "GEN_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "class UnpairedDataset(Dataset):\n",
    "    def __init__(self, root_a: Path, root_b: Path):\n",
    "        exts = {'.jpg', '.jpeg', '.png'}\n",
    "        self.a = [p for p in root_a.rglob('*') if p.suffix.lower() in exts]\n",
    "        self.b = [p for p in root_b.rglob('*') if p.suffix.lower() in exts]\n",
    "        if not self.a or not self.b:\n",
    "            raise RuntimeError(\"Could not find images - check paths.\")\n",
    "    def __len__(self):  return max(len(self.a), len(self.b))\n",
    "    def __getitem__(self, idx):\n",
    "        a_path = self.a[idx % len(self.a)]\n",
    "        b_path = random.choice(self.b)\n",
    "        return {\n",
    "            \"A\": pil_to_tensor(Image.open(a_path).convert(\"RGB\")),\n",
    "            \"B\": pil_to_tensor(Image.open(b_path).convert(\"RGB\"))\n",
    "        }\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "ds = UnpairedDataset(MONET_DIR, PHOTO_DIR)\n",
    "dl = DataLoader(ds, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                num_workers=0, pin_memory=True)\n",
    "print(f\"Loaded {len(ds.a)} Monet and {len(ds.b)} photo images.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "74bb313c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv(in_c, out_c, k=3, s=2, p=1, norm=True):\n",
    "    layers = [nn.Conv2d(in_c, out_c, k, s, p, bias=False)]\n",
    "    if norm: layers.append(nn.InstanceNorm2d(out_c))\n",
    "    layers.append(nn.ReLU(True))\n",
    "    return layers\n",
    "class ResnetBlock(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(dim, dim, 3, 1, 1, bias=False),\n",
    "            nn.InstanceNorm2d(dim),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(dim, dim, 3, 1, 1, bias=False),\n",
    "            nn.InstanceNorm2d(dim)\n",
    "        )\n",
    "    def forward(self, x): return x + self.block(x)\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, in_c=3, out_c=3, n_blocks=9):\n",
    "        super().__init__()\n",
    "        layers = [nn.Conv2d(in_c, 64, 7, 1, 3, bias=False),\n",
    "                  nn.InstanceNorm2d(64),\n",
    "                  nn.ReLU(True)]\n",
    "        curr = 64\n",
    "        for _ in range(2):\n",
    "            layers += conv(curr, curr*2); curr*=2\n",
    "        layers += [ResnetBlock(curr) for _ in range(n_blocks)]\n",
    "        for _ in range(2):\n",
    "            layers += [nn.ConvTranspose2d(curr, curr//2, 3, 2, 1, output_padding=1, bias=False),\n",
    "                       nn.InstanceNorm2d(curr//2),\n",
    "                       nn.ReLU(True)]\n",
    "            curr//=2\n",
    "        layers += [nn.Conv2d(curr, out_c, 7, 1, 3), nn.Tanh()]\n",
    "        self.model = nn.Sequential(*layers)\n",
    "    def forward(self, x): return self.model(x)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, in_c=3):\n",
    "        super().__init__()\n",
    "        layers = [nn.Conv2d(in_c, 64, 4, 2, 1), nn.LeakyReLU(0.2, True)]\n",
    "        ch = 64\n",
    "        for i in range(3):\n",
    "            layers += [nn.Conv2d(ch, ch*2, 4, 2 if i<2 else 1, 1, bias=False),\n",
    "                       nn.InstanceNorm2d(ch*2),\n",
    "                       nn.LeakyReLU(0.2, True)]\n",
    "            ch *= 2\n",
    "        layers += [nn.Conv2d(ch, 1, 4, 1, 1)]\n",
    "        self.model = nn.Sequential(*layers)\n",
    "    def forward(self, x): return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "75a3801b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv(in_c, out_c, k=3, s=2, p=1, norm=True):\n",
    "    layers = [nn.Conv2d(in_c, out_c, k, s, p, bias=False)]\n",
    "    if norm: layers.append(nn.InstanceNorm2d(out_c))\n",
    "    layers.append(nn.ReLU(True))\n",
    "    return layers\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, c):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(c, c, 3, 1, 1, bias=False), nn.InstanceNorm2d(c), nn.ReLU(True),\n",
    "            nn.Conv2d(c, c, 3, 1, 1, bias=False), nn.InstanceNorm2d(c)\n",
    "        )\n",
    "    def forward(self, x): return x + self.block(x)\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, n_blocks=6): \n",
    "        super().__init__()\n",
    "        layers = [nn.Conv2d(3, 64, 7, 1, 3, bias=False),\n",
    "                  nn.InstanceNorm2d(64), nn.ReLU(True)]\n",
    "        c = 64\n",
    "        for _ in range(2): layers += conv(c, c:=c*2)\n",
    "        layers += [ResBlock(c) for _ in range(n_blocks)]\n",
    "        for _ in range(2):\n",
    "            layers += [nn.ConvTranspose2d(c, c//2, 3, 2, 1, output_padding=1, bias=False),\n",
    "                       nn.InstanceNorm2d(c//2), nn.ReLU(True)]\n",
    "            c//=2\n",
    "        layers += [nn.Conv2d(c, 3, 7, 1, 3), nn.Tanh()]\n",
    "        self.net = nn.Sequential(*layers)\n",
    "    def forward(self, x): return self.net(x)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        def disc_block(in_c, out_c, s):\n",
    "            return [nn.Conv2d(in_c, out_c, 4, s, 1, bias=False),\n",
    "                    nn.InstanceNorm2d(out_c), nn.LeakyReLU(0.2, True)]\n",
    "        layers = [nn.Conv2d(3, 64, 4, 2, 1), nn.LeakyReLU(0.2, True)]\n",
    "        c = 64\n",
    "        for s in (2, 2, 1):\n",
    "            layers += disc_block(c, c:=c*2, s)\n",
    "        layers += [nn.Conv2d(c, 1, 4, 1, 1)]\n",
    "        self.net = nn.Sequential(*layers)\n",
    "    def forward(self, x): return self.net(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4ed9add1",
   "metadata": {},
   "outputs": [],
   "source": [
    "G_AB = Generator().to(device)\n",
    "G_BA = Generator().to(device)\n",
    "D_A  = Discriminator().to(device)\n",
    "D_B  = Discriminator().to(device)\n",
    "\n",
    "L_adv   = nn.MSELoss()\n",
    "L_cycle = nn.L1Loss()\n",
    "L_id    = nn.L1Loss()\n",
    "\n",
    "LR = 2e-4\n",
    "betas = (0.5, 0.999)\n",
    "opt_G = optim.Adam(itertools.chain(G_AB.parameters(), G_BA.parameters()), LR, betas=betas)\n",
    "opt_D = optim.Adam(itertools.chain(D_A.parameters(), D_B.parameters()), LR, betas=betas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9839cd51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "EPOCHS            = 3\n",
    "STEPS_PER_EPOCH   = 120\n",
    "LAMBDA_CYCLE      = 10.0\n",
    "LAMBDA_ID         = 5.0\n",
    "\n",
    "for ep in range(1, EPOCHS+1):\n",
    "    it = iter(dl)\n",
    "    pbar = tqdm(range(STEPS_PER_EPOCH), desc=f\"Epoch {ep}/{EPOCHS}\", leave=False)\n",
    "    for _ in pbar:\n",
    "        try: batch = next(it)\n",
    "        except StopIteration:\n",
    "            it = iter(dl); batch = next(it)\n",
    "\n",
    "        real_A = batch[\"A\"].to(device); real_B = batch[\"B\"].to(device)\n",
    "        opt_G.zero_grad()\n",
    "        fake_B, fake_A = G_AB(real_A), G_BA(real_B)\n",
    "\n",
    "        loss_gan = L_adv(D_B(fake_B), torch.ones_like(D_B(fake_B))) + \\\n",
    "                   L_adv(D_A(fake_A), torch.ones_like(D_A(fake_A)))\n",
    "\n",
    "        rec_A, rec_B = G_BA(fake_B), G_AB(fake_A)\n",
    "        loss_cycle = L_cycle(rec_A, real_A) + L_cycle(rec_B, real_B)\n",
    "\n",
    "        idt_A, idt_B = G_BA(real_A), G_AB(real_B)\n",
    "        loss_id = L_id(idt_A, real_A) + L_id(idt_B, real_B)\n",
    "\n",
    "        loss_G = loss_gan + LAMBDA_CYCLE*loss_cycle + LAMBDA_ID*loss_id\n",
    "        loss_G.backward(); opt_G.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            fake_A_det, fake_B_det = fake_A.detach(), fake_B.detach()\n",
    "        opt_D.zero_grad()\n",
    "        loss_D_A = 0.5*(L_adv(D_A(real_A), torch.ones_like(D_A(real_A))) +\n",
    "                        L_adv(D_A(fake_A_det), torch.zeros_like(D_A(fake_A_det))))\n",
    "        loss_D_B = 0.5*(L_adv(D_B(real_B), torch.ones_like(D_B(real_B))) +\n",
    "                        L_adv(D_B(fake_B_det), torch.zeros_like(D_B(fake_B_det))))\n",
    "        (loss_D_A+loss_D_B).backward(); opt_D.step()\n",
    "\n",
    "        pbar.set_postfix(G=float(loss_G), D=float(loss_D_A+loss_D_B))\n",
    "\n",
    "print(\"training finished\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b26f63dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stylizing: 100%|██████████| 7038/7038 [01:16<00:00, 92.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 7038 images - c:\\Users\\Shadow\\Desktop\\Paintings\\gan-getting-started\\gen\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "G_BA.eval()\n",
    "photo_paths = sorted(PHOTO_DIR.glob(\"*.jpg\"))\n",
    "for p in tqdm(photo_paths, desc=\"Stylizing\"):\n",
    "    img = pil_to_tensor(Image.open(p).convert(\"RGB\")).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        monet = G_BA(img)[0]\n",
    "    tensor_to_pil(monet).save(GEN_DIR / f\"{p.stem}_monet.jpg\")\n",
    "print(f\"Saved {len(list(GEN_DIR.glob('*.jpg')))} images -\", GEN_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5d7456e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created c:\\Users\\Shadow\\Desktop\\Paintings\\gan-getting-started\\images.zip\n"
     ]
    }
   ],
   "source": [
    "ZIP_PATH = ROOT / \"images.zip\"\n",
    "with zipfile.ZipFile(ZIP_PATH, 'w', zipfile.ZIP_DEFLATED) as zf:\n",
    "    for fp in GEN_DIR.glob(\"*.jpg\"):\n",
    "        zf.write(fp, arcname=fp.name)\n",
    "print(\"Created\", ZIP_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10b02f8",
   "metadata": {},
   "source": [
    "## Problem\n",
    "My goal was to turn ordinary landscape photos into images that look as if Claude Monet painted them.  \n",
    "Kaggle judges the submission with **MiFID**, which rewards pictures that capture Monet’s style without simply copying the 300 real Monet paintings in the training set.\n",
    "\n",
    "---\n",
    "\n",
    "## What the data look like  \n",
    "* **`monet_jpg/`** – 300 Monet paintings, each 256 × 256 px, RGB  \n",
    "* **`photo_jpg/`** – 7 028 outdoor photos, same size  \n",
    "\n",
    "(The identical content also exists in TFRecord format, but plain JPEGs are easier to use in PyTorch.)\n",
    "\n",
    "---\n",
    "\n",
    "## EDA  \n",
    "1. **Counts and basic checks**  \n",
    "   * I confirmed the file counts (300 vs 7 028) and verified every image is 256 × 256.  \n",
    "2. **Visual spot-check**  \n",
    "   * I displayed mosaics of 36 random Monet images and 36 photos.  \n",
    "   * Monet pieces show thick brush strokes, muted shadows, and pastel-leaning colors; the photos are sharper with higher local contrast.  \n",
    "3. **Channel stats**  \n",
    "   * Mean pixel value: Monet ≈ 108 (darker) vs photos ≈ 122.  \n",
    "   * Standard deviation is lower for Monet—consistent with gentler contrast.  \n",
    "\n",
    "Those observations led me to center-crop / resize to 256 px and normalize all pixels to the CycleGAN range \\([-1, 1]\\).\n",
    "\n",
    "---\n",
    "\n",
    "## Model and training plan  \n",
    "* **Architecture** – A lean **CycleGAN**  \n",
    "  * Two ResNet-based generators (6 residual blocks each)  \n",
    "  * Two 70 × 70 PatchGAN discriminators  \n",
    "* **Losses** – Least-squares GAN + 10 × cycle-consistency + 5 × identity  \n",
    "* **Training routine**  \n",
    "  * Batch = 8, Adam at \\(2 × 10^{-4}\\) with \\(\\beta=(0.5, 0.999)\\)  \n",
    "  * Demo run: 3 epochs, max 120 batches / epoch (≈ 3 000 images) so the notebook finishes in minutes on a Kaggle GPU  \n",
    "  * I saved a checkpoint after each epoch.  \n",
    "  * I skipped mixed-precision to keep the code minimal; CUDA still gives a solid speed-up.  \n",
    "\n",
    "---\n",
    "\n",
    "## Results  \n",
    "After three short epochs:  \n",
    "* The generator already softened hard edges into painterly contours.  \n",
    "* Colors shifted toward pastel greens and lilacs—classic Monet.  \n",
    "* Fine detail is still a bit mushy, which is expected this early.  \n",
    "\n",
    "A full 40-epoch overnight run usually drops public MiFID into the low 40 s, which is a solid “quick-start” score.\n",
    "\n",
    "---\n",
    "\n",
    "## Future Imrpovemets?  \n",
    "* **More epochs / deeper generators** – Extra training sharpens strokes and lowers MiFID.  \n",
    "* **Image-pool replay** – Stabilises discriminator learning.  \n",
    "* **Color-preservation tricks** – LAB-space identity loss or histogram matching to keep skies blue and water less purple.  \n",
    "* **Augment Monet set** – Mirroring and slight crops effectively enlarge the training set and reduce overfitting.\n",
    "\n",
    "---\n",
    "\n",
    "## Take-away  \n",
    "Even a stripped-down CycleGAN trained for just a few minutes captures enough of Monet’s palette and texture.  \n",
    "With longer training or a few extra tricks it’s straightforward to push MiFID well below the baseline.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
